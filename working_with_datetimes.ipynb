{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serious-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pandas\n",
    "# !pip3 install pytz\n",
    "import pandas as pd\n",
    "import pytz  # we just need this for assigning the timezones later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-clarity",
   "metadata": {},
   "source": [
    "In this blog post I will be summarising some of the more useful datetime functions in pandas.\n",
    "\n",
    "In my current job, I often find myself dealing with time series datasets which require various datetime transformations. Initially I found myself spending a lot of time jumping around the [datetime](https://docs.python.org/3/library/datetime.html) and [pandas](https://pandas.pydata.org/docs/user_guide/index.html) documentation trying to find out how to perform some of these transformations which took up a lot of time. With that in mind, this blog post aims to provide code which you can lift into your own scripts or notebooks when working with datetimes in pandas.\n",
    "\n",
    "This post will be split into two sections. In section 1, I will simply summarise some of the functions I find myself using most often with a simple sample dataset I created. The code in this section can act as a quick copy \\& paste for you if you are just looking to find out how to perform a particular datetime transformation.\n",
    "\n",
    "In the section 2, I will show how the code snippets from section 1 can be pulled together and used in a real world example on the [Emergency - 911 Calls dataset](https://www.kaggle.com/mchirico/montcoalert).\n",
    "\n",
    "Before we get stuck into things, it's worth pointing out that really most important thing to remember when working with datetimes in pandas is that using `.dt` opens up access to most of the datetime functions in python (and even some extra ones). This will become clearer as we work through the examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-runner",
   "metadata": {},
   "source": [
    "# Useful snippets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-prayer",
   "metadata": {},
   "source": [
    "## The dataset \n",
    "\n",
    "The dataset I will use in this section will be a simple dataset I created which tracks the time between users logging into a website and them purchasing something on the site.\n",
    "\n",
    "In this dataset, `session_start_date_time` tacks when the user logged into the site, `sale_date_time` tracks when the time the user purchased something on the website, and `sale_value` denotes the value of the sale (for this example the currency doesn't matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "intelligent-cardiff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date_time</th>\n",
       "      <th>sale_date_time</th>\n",
       "      <th>sale_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01T04:44:00</td>\n",
       "      <td>2019-01-01T05:17:00</td>\n",
       "      <td>37.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01T05:12:00</td>\n",
       "      <td>2019-01-01T05:33:00</td>\n",
       "      <td>37.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01T05:23:00</td>\n",
       "      <td>2019-01-01T05:42:00</td>\n",
       "      <td>33.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01T05:35:00</td>\n",
       "      <td>2019-01-01T05:44:00</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01T05:23:00</td>\n",
       "      <td>2019-01-01T05:54:00</td>\n",
       "      <td>26.233333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  session_start_date_time       sale_date_time  sale_value\n",
       "0     2019-01-01T04:44:00  2019-01-01T05:17:00   37.166667\n",
       "1     2019-01-01T05:12:00  2019-01-01T05:33:00   37.766667\n",
       "2     2019-01-01T05:23:00  2019-01-01T05:42:00   33.533333\n",
       "3     2019-01-01T05:35:00  2019-01-01T05:44:00   43.000000\n",
       "4     2019-01-01T05:23:00  2019-01-01T05:54:00   26.233333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pytz  # we just need this for assigning the timezones later\n",
    "\n",
    "time_data = pd.read_csv(\"sample_data.csv\")\n",
    "time_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-refund",
   "metadata": {},
   "source": [
    "## Convert timestamps to datetimes\n",
    "\n",
    "Before we can get started with datetime transformations, we must first convert our timestamp columns to datetime objects. To convert timestamps into a datetime object we can simply use the `pd.to_datetime(<YOUR_COLUMN>)` function.\n",
    "\n",
    "It's worth noting that this step is very important when dealing with timestamps in pandas as without converting them to datetimes, all the nice datetime functionality within pandas will not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "hundred-scale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date_time</th>\n",
       "      <th>sale_date_time</th>\n",
       "      <th>sale_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 04:44:00</td>\n",
       "      <td>2019-01-01 05:17:00</td>\n",
       "      <td>37.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 05:12:00</td>\n",
       "      <td>2019-01-01 05:33:00</td>\n",
       "      <td>37.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 05:23:00</td>\n",
       "      <td>2019-01-01 05:42:00</td>\n",
       "      <td>33.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 05:35:00</td>\n",
       "      <td>2019-01-01 05:44:00</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05:23:00</td>\n",
       "      <td>2019-01-01 05:54:00</td>\n",
       "      <td>26.233333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  session_start_date_time      sale_date_time  sale_value\n",
       "0     2019-01-01 04:44:00 2019-01-01 05:17:00   37.166667\n",
       "1     2019-01-01 05:12:00 2019-01-01 05:33:00   37.766667\n",
       "2     2019-01-01 05:23:00 2019-01-01 05:42:00   33.533333\n",
       "3     2019-01-01 05:35:00 2019-01-01 05:44:00   43.000000\n",
       "4     2019-01-01 05:23:00 2019-01-01 05:54:00   26.233333"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_date_time\"] = pd.to_datetime(time_data[\"session_start_date_time\"])\n",
    "time_data[\"sale_date_time\"] = pd.to_datetime(time_data[\"sale_date_time\"])\n",
    "time_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-latin",
   "metadata": {},
   "source": [
    "## Subtracting datetimes \n",
    "\n",
    "Once we have converted our timestamps to datetimes, we can very easily determine the `time_to_sale` for each session by subtracting `session_start_date_time` from the `sale_date_time`.\n",
    "\n",
    "This operation creates a column of [timedeltas](https://docs.python.org/3/library/datetime.html#datetime.timedelta) which denote the difference in terms of days, hours, minutes, and seconds between the two datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "prostate-findings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date_time</th>\n",
       "      <th>sale_date_time</th>\n",
       "      <th>sale_value</th>\n",
       "      <th>time_to_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 04:44:00</td>\n",
       "      <td>2019-01-01 05:17:00</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>0 days 00:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 05:12:00</td>\n",
       "      <td>2019-01-01 05:33:00</td>\n",
       "      <td>37.766667</td>\n",
       "      <td>0 days 00:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 05:23:00</td>\n",
       "      <td>2019-01-01 05:42:00</td>\n",
       "      <td>33.533333</td>\n",
       "      <td>0 days 00:19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 05:35:00</td>\n",
       "      <td>2019-01-01 05:44:00</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0 days 00:09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05:23:00</td>\n",
       "      <td>2019-01-01 05:54:00</td>\n",
       "      <td>26.233333</td>\n",
       "      <td>0 days 00:31:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  session_start_date_time      sale_date_time  sale_value    time_to_sale\n",
       "0     2019-01-01 04:44:00 2019-01-01 05:17:00   37.166667 0 days 00:33:00\n",
       "1     2019-01-01 05:12:00 2019-01-01 05:33:00   37.766667 0 days 00:21:00\n",
       "2     2019-01-01 05:23:00 2019-01-01 05:42:00   33.533333 0 days 00:19:00\n",
       "3     2019-01-01 05:35:00 2019-01-01 05:44:00   43.000000 0 days 00:09:00\n",
       "4     2019-01-01 05:23:00 2019-01-01 05:54:00   26.233333 0 days 00:31:00"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"time_to_sale\"] = time_data[\"sale_date_time\"] - time_data[\"session_start_date_time\"]\n",
    "time_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "destroyed-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = time_data.drop(\"time_to_sale\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-auditor",
   "metadata": {},
   "source": [
    "## Timezones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-newton",
   "metadata": {},
   "source": [
    "### Assigning a timezone \n",
    "\n",
    "Oftentimes I find myself producing outputs for people in different parts of the world. As a result of this it is important to be able to present any time related data outputs (such as visualisations) in the end users timezone, especially when they don't come from a technical background.\n",
    "\n",
    "The first step in being able to perform any timezone conversions is to localise the data to the timezone it came from. Now typically if you are storing data in a database, it will be in UTC ISO format, so the `pd.to_dateime()` function will infer the timezone from it when converting to a datetime. However, in our case we don't have any timezone information for these timestamps, so we can go ahead and localise it to UTC using the `tz_localize()` function. \n",
    "\n",
    "This function will accept a timezone object (in this case we pass a [pytz](https://pypi.org/project/pytz/) timezone) and will localise the column to that timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legal-karen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date_time</th>\n",
       "      <th>sale_date_time</th>\n",
       "      <th>sale_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 04:44:00+00:00</td>\n",
       "      <td>2019-01-01 05:17:00+00:00</td>\n",
       "      <td>37.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 05:12:00+00:00</td>\n",
       "      <td>2019-01-01 05:33:00+00:00</td>\n",
       "      <td>37.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:42:00+00:00</td>\n",
       "      <td>33.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 05:35:00+00:00</td>\n",
       "      <td>2019-01-01 05:44:00+00:00</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:54:00+00:00</td>\n",
       "      <td>26.233333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session_start_date_time            sale_date_time  sale_value\n",
       "0 2019-01-01 04:44:00+00:00 2019-01-01 05:17:00+00:00   37.166667\n",
       "1 2019-01-01 05:12:00+00:00 2019-01-01 05:33:00+00:00   37.766667\n",
       "2 2019-01-01 05:23:00+00:00 2019-01-01 05:42:00+00:00   33.533333\n",
       "3 2019-01-01 05:35:00+00:00 2019-01-01 05:44:00+00:00   43.000000\n",
       "4 2019-01-01 05:23:00+00:00 2019-01-01 05:54:00+00:00   26.233333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_date_time\"] = time_data[\"session_start_date_time\"].dt.tz_localize(pytz.utc)\n",
    "time_data[\"sale_date_time\"] = time_data[\"sale_date_time\"].dt.tz_localize(pytz.utc)\n",
    "time_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-estate",
   "metadata": {},
   "source": [
    "### Converting to a different timezone \n",
    "\n",
    "Once we have our timestamps localised to UTC (or any timezone), we can easily switch between timezones using `tz_convert()`. Once again, this function will accept a timezone object, and will convert our column to to that timezone.\n",
    "\n",
    "In the below example we convert the `session_start_date_time` to Berlin local time.\n",
    "\n",
    "For your reference, a list of pytz supported timezones can be found by running `pytz.all_timezones` within a python shell or jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "immediate-newark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2019-01-01 05:44:00+01:00\n",
       "1   2019-01-01 06:12:00+01:00\n",
       "2   2019-01-01 06:23:00+01:00\n",
       "3   2019-01-01 06:35:00+01:00\n",
       "4   2019-01-01 06:23:00+01:00\n",
       "Name: session_start_date_time, dtype: datetime64[ns, Europe/Berlin]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_date_time\"].dt.tz_convert(pytz.timezone(\"Europe/Berlin\")).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-provider",
   "metadata": {},
   "source": [
    "### Removing timezone info \n",
    "\n",
    "If for some reason you want to remove the timezone information from a column, if can easily be done by once again just using `tz_localize`, except instead of passing a timezone, just pass `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-bonus",
   "metadata": {},
   "source": [
    "## Convert a datetime to ISO format\n",
    "\n",
    "If you're working on a data transformation workflow involving timestamps that will end up being ingested into a database, more often than not you will need to convert your timestamps into [ISO format](https://en.wikipedia.org/wiki/ISO_8601).\n",
    "\n",
    "Although this is one of the only datetime transformations we cannot do in pandas by accesing the functions within `.dt`, we can convert datetimes to ISO format using an `apply` on the column and accessing the `isoformat` method on each datetime object, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "changing-ferry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2019-01-01T04:44:00+00:00\n",
       "1    2019-01-01T05:12:00+00:00\n",
       "2    2019-01-01T05:23:00+00:00\n",
       "3    2019-01-01T05:35:00+00:00\n",
       "4    2019-01-01T05:23:00+00:00\n",
       "Name: session_start_date_time, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_date_time\"].apply(lambda x: x.isoformat()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-workshop",
   "metadata": {},
   "source": [
    "## Adding a timedelta or offset to a datetime\n",
    "\n",
    "Sometimes we might want to add an offset to a datetime in pandas (e.g. create a `time_interval_end` column by adding 5 minnutes to a `time_interval_start`). To do this, we simply add a timedelta to the a datetime column using the `pd.to_timedlta()` function.\n",
    "\n",
    "In the below example, we add 15 minutes to the `session_start_date_time` using this function. More information on the inputs to this function can be found [here](https://pandas.pydata.org/docs/reference/api/pandas.to_timedelta.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hourly-favorite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date_time</th>\n",
       "      <th>session_start_plus_fifteen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 04:44:00+00:00</td>\n",
       "      <td>2019-01-01 04:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 05:12:00+00:00</td>\n",
       "      <td>2019-01-01 05:27:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:38:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 05:35:00+00:00</td>\n",
       "      <td>2019-01-01 05:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:38:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session_start_date_time session_start_plus_fifteen\n",
       "0 2019-01-01 04:44:00+00:00  2019-01-01 04:59:00+00:00\n",
       "1 2019-01-01 05:12:00+00:00  2019-01-01 05:27:00+00:00\n",
       "2 2019-01-01 05:23:00+00:00  2019-01-01 05:38:00+00:00\n",
       "3 2019-01-01 05:35:00+00:00  2019-01-01 05:50:00+00:00\n",
       "4 2019-01-01 05:23:00+00:00  2019-01-01 05:38:00+00:00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_plus_fifteen\"] = time_data[\"session_start_date_time\"] + pd.to_timedelta(15,'m')\n",
    "time_data[[\"session_start_date_time\", \"session_start_plus_fifteen\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fuzzy-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = time_data.drop(\"session_start_plus_fifteen\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-comparison",
   "metadata": {},
   "source": [
    "## Rounding datetimes \n",
    "\n",
    "Imagine a situation where you want to create a report or visualisation of number of sessions per hour, 15 minute, or 5 minute interval. The first step in generating this would be to round our timestamps up or down (depending on how you want to do it) to the nearest interval. We can do this in pandas using the `.dt.ceil()` and `.dt.floor()` methods.\n",
    "\n",
    "In the below examples, you will see how we can round the `session_start_date_time` up or down to the nearest 5 minute interval using the methods mentioned above. A summary on how to format the input to these functions to round to different time intervals can be found [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases).\n",
    "\n",
    "It's worth noting that this kind of rounding functionality is actually not available within the base datetime library and sometimes if you want to round a single datetime it can be handy to convert it to a series and apply this function before converting it back to a datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stylish-worcester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date_time</th>\n",
       "      <th>session_start_rounded_ceiling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 04:44:00+00:00</td>\n",
       "      <td>2019-01-01 04:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 05:12:00+00:00</td>\n",
       "      <td>2019-01-01 05:15:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:25:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 05:35:00+00:00</td>\n",
       "      <td>2019-01-01 05:35:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:25:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session_start_date_time session_start_rounded_ceiling\n",
       "0 2019-01-01 04:44:00+00:00     2019-01-01 04:45:00+00:00\n",
       "1 2019-01-01 05:12:00+00:00     2019-01-01 05:15:00+00:00\n",
       "2 2019-01-01 05:23:00+00:00     2019-01-01 05:25:00+00:00\n",
       "3 2019-01-01 05:35:00+00:00     2019-01-01 05:35:00+00:00\n",
       "4 2019-01-01 05:23:00+00:00     2019-01-01 05:25:00+00:00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_rounded_ceiling\"] = time_data[\"session_start_date_time\"].dt.ceil(\"5min\")\n",
    "time_data[[\"session_start_date_time\", \"session_start_rounded_ceiling\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incident-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date_time</th>\n",
       "      <th>session_start_rounded_floor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 04:44:00+00:00</td>\n",
       "      <td>2019-01-01 04:40:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 05:12:00+00:00</td>\n",
       "      <td>2019-01-01 05:10:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:20:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 05:35:00+00:00</td>\n",
       "      <td>2019-01-01 05:35:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05:23:00+00:00</td>\n",
       "      <td>2019-01-01 05:20:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session_start_date_time session_start_rounded_floor\n",
       "0 2019-01-01 04:44:00+00:00   2019-01-01 04:40:00+00:00\n",
       "1 2019-01-01 05:12:00+00:00   2019-01-01 05:10:00+00:00\n",
       "2 2019-01-01 05:23:00+00:00   2019-01-01 05:20:00+00:00\n",
       "3 2019-01-01 05:35:00+00:00   2019-01-01 05:35:00+00:00\n",
       "4 2019-01-01 05:23:00+00:00   2019-01-01 05:20:00+00:00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_rounded_floor\"] = time_data[\"session_start_date_time\"].dt.floor(\"5min\")\n",
    "time_data[[\"session_start_date_time\", \"session_start_rounded_floor\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-recycling",
   "metadata": {},
   "source": [
    "## Generate a list of datetimes at a particular frequency \n",
    "\n",
    "Again, if you want to generate some aggregation of sessions per 5 minute (or hour, or 15 minute) interval, it is useful to first generate a set of all intervals which exist between the min and max times of your data. Failure to do this could mean that your aggregation would not represent intervals for which no sessions were started.\n",
    "\n",
    "Thankfully in pandas we can the the `pd.date_range()` function to generate a series of timestamps between two datetimes at a particular interval. \n",
    "\n",
    "In the below example, you will notice that we convert the output of this function to a dataframe. This is something I like to do as it makes it easier to join on the aggregated data to take account of these empty intervals, this will become more clear in the worked example in section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "forward-correlation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 04:40:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 04:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 04:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 04:55:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              time_interval\n",
       "0 2019-01-01 04:40:00+00:00\n",
       "1 2019-01-01 04:45:00+00:00\n",
       "2 2019-01-01 04:50:00+00:00\n",
       "3 2019-01-01 04:55:00+00:00\n",
       "4 2019-01-01 05:00:00+00:00"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals = pd.DataFrame(\n",
    "    pd.date_range(\n",
    "        start=time_data[\"session_start_rounded_floor\"].min(),\n",
    "        end=time_data[\"session_start_rounded_ceiling\"].max(),\n",
    "        freq=\"5min\"\n",
    "    ),\n",
    "    columns=[\"time_interval\"]\n",
    ")\n",
    "intervals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-meeting",
   "metadata": {},
   "source": [
    "## Group by date or week\n",
    "\n",
    "If we want to look at the data as a more summarised level (e.g. date or week), we don't have to use the `ceil()` or `floor()` functions. We can instead used the built in `date()` and `isocalendar().week()` functions to before using a `groupby` to aggregate the data by date or week number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bronze-australia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_date</th>\n",
       "      <th>session_start_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  session_start_date  session_start_date_time\n",
       "0         2019-01-01                      838\n",
       "1         2019-01-02                      935\n",
       "2         2019-01-03                      905\n",
       "3         2019-01-04                      909\n",
       "4         2019-01-05                      717"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_date\"] = time_data[\"session_start_date_time\"].dt.date\n",
    "time_data.groupby(\"session_start_date\")[\"session_start_date_time\"].count().reset_index().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "removable-namibia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_start_week</th>\n",
       "      <th>session_start_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_start_week  session_start_date_time\n",
       "0                   1                     6845\n",
       "1                   2                     6066\n",
       "2                   3                     5949\n",
       "3                   4                     5953\n",
       "4                   5                     5767"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice we get the week number here\n",
    "time_data[\"session_start_week\"] = time_data[\"session_start_date_time\"].dt.isocalendar().week\n",
    "time_data.groupby(\"session_start_week\")[\"session_start_date_time\"].count().reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-worry",
   "metadata": {},
   "source": [
    "## Formatting datetimes as a string\n",
    "\n",
    "Finally, sometimes it is useful, particularly when plotting aggregated time data to be able to format the datetimes nicely (e.g. remove the seconds and convert the month number to the shorthand month). To do this we can use the `.dt.strftime()` function to format the time into our desired format.\n",
    "\n",
    "A list of formatting options can be found [here](https://www.programiz.com/python-programming/datetime/strftime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alike-bride",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    01-Jan-19 04:44\n",
       "1    01-Jan-19 05:12\n",
       "2    01-Jan-19 05:23\n",
       "3    01-Jan-19 05:35\n",
       "4    01-Jan-19 05:23\n",
       "Name: session_start_date_time, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_data[\"session_start_date_time\"].dt.strftime('%d-%b-%y %H:%M').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-wound",
   "metadata": {},
   "source": [
    "# A worked example\n",
    "\n",
    "Consider a situation where you work for the Montgomery County, PA police department and your new boss provides you with a csv extract of all 911 calls from 2015 to 2020 as they are keen to understand to get up to speed on the general trends over the last few years.\n",
    "\n",
    "In particular they want to get and understanding of the busiest week in the year, and within that week what was the busiest 30 minute interval. They would also like a plot of 991 volume for all the 30 minute intervals that week to better understand just how busy it was as the busiest time.\n",
    "\n",
    "Finally, they were requested to send on a summary of their data (timestamp, desc, title, and twp) to the FBI for upload to the federal database, however the FBI requested that the timestamps be sent on as UTC times in ISO format in order to comply with their database rules. This will mean that you will have to convert the times from local (PA) time to UTC before doing the ISO formatting.\n",
    "\n",
    "We can break down this request into the following 5 steps:\n",
    "\n",
    "- Localise the timestamps to the local (Pennsylvania) time (US/Eastern)\n",
    "- Find out which week had the most 911 calls\n",
    "- Identify which 30 minute window over that week had the most 911 calls?\n",
    "- What was driving the volume at the time interval with the highest volume?\n",
    "- Plot all the 30 minute intervals for that week\n",
    "- Export the UTC time data to a csv in ISO format (timestamp, desc, title, twp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-annual",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "To begin, we will import the data, convert the `timestamp` column to a datetime object, and check for any `NULL`/`None` values. Thankfully for us there are actually no nulls in the dataset so no action is required there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corrected-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nulls in dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>title</th>\n",
       "      <th>twp</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REINDEER CT &amp; DEAD END;  NEW HANOVER; Station ...</td>\n",
       "      <td>EMS: BACK PAINS/INJURY</td>\n",
       "      <td>NEW HANOVER</td>\n",
       "      <td>2015-12-10 17:10:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRIAR PATH &amp; WHITEMARSH LN;  HATFIELD TOWNSHIP...</td>\n",
       "      <td>EMS: DIABETIC EMERGENCY</td>\n",
       "      <td>HATFIELD TOWNSHIP</td>\n",
       "      <td>2015-12-10 17:29:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...</td>\n",
       "      <td>Fire: GAS-ODOR/LEAK</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "      <td>2015-12-10 14:39:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIRY ST &amp; SWEDE ST;  NORRISTOWN; Station 308A;...</td>\n",
       "      <td>EMS: CARDIAC EMERGENCY</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "      <td>2015-12-10 16:47:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHERRYWOOD CT &amp; DEAD END;  LOWER POTTSGROVE; S...</td>\n",
       "      <td>EMS: DIZZINESS</td>\n",
       "      <td>LOWER POTTSGROVE</td>\n",
       "      <td>2015-12-10 16:56:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                desc                    title  \\\n",
       "0  REINDEER CT & DEAD END;  NEW HANOVER; Station ...   EMS: BACK PAINS/INJURY   \n",
       "1  BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP...  EMS: DIABETIC EMERGENCY   \n",
       "2  HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...      Fire: GAS-ODOR/LEAK   \n",
       "3  AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A;...   EMS: CARDIAC EMERGENCY   \n",
       "4  CHERRYWOOD CT & DEAD END;  LOWER POTTSGROVE; S...           EMS: DIZZINESS   \n",
       "\n",
       "                 twp           timestamp  \n",
       "0        NEW HANOVER 2015-12-10 17:10:52  \n",
       "1  HATFIELD TOWNSHIP 2015-12-10 17:29:21  \n",
       "2         NORRISTOWN 2015-12-10 14:39:21  \n",
       "3         NORRISTOWN 2015-12-10 16:47:36  \n",
       "4   LOWER POTTSGROVE 2015-12-10 16:56:52  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data = pd.read_csv(\"911.csv\")\n",
    "example_data[\"timestamp\"] = pd.to_datetime(example_data[\"timeStamp\"])\n",
    "\n",
    "# i don't like the camel casing here so removing along with some other columns we won't need\n",
    "example_data = example_data.drop([\"timeStamp\", \"lat\", \"lng\", \"zip\", \"addr\", \"e\"], axis=1) \n",
    "print(f\"{len(example_data.loc[example_data['timestamp'].isna()])} nulls in dataset\")\n",
    "example_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-bailey",
   "metadata": {},
   "source": [
    "## Localise timestamps to the UTC then convert to PA local time \n",
    "\n",
    "From here, we can loalise the timestamps to US Eastern time as this is what the timestamps were recorded as in the original data. Note that we set `ambiguous=True` when making this localisation as the data doesn't include any information on daylight savings offset so we have to tell the function to ignore this as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heavy-favorite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>title</th>\n",
       "      <th>twp</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestamp_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REINDEER CT &amp; DEAD END;  NEW HANOVER; Station ...</td>\n",
       "      <td>EMS: BACK PAINS/INJURY</td>\n",
       "      <td>NEW HANOVER</td>\n",
       "      <td>2015-12-10 17:10:52</td>\n",
       "      <td>2015-12-10 17:10:52-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRIAR PATH &amp; WHITEMARSH LN;  HATFIELD TOWNSHIP...</td>\n",
       "      <td>EMS: DIABETIC EMERGENCY</td>\n",
       "      <td>HATFIELD TOWNSHIP</td>\n",
       "      <td>2015-12-10 17:29:21</td>\n",
       "      <td>2015-12-10 17:29:21-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...</td>\n",
       "      <td>Fire: GAS-ODOR/LEAK</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "      <td>2015-12-10 14:39:21</td>\n",
       "      <td>2015-12-10 14:39:21-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIRY ST &amp; SWEDE ST;  NORRISTOWN; Station 308A;...</td>\n",
       "      <td>EMS: CARDIAC EMERGENCY</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "      <td>2015-12-10 16:47:36</td>\n",
       "      <td>2015-12-10 16:47:36-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHERRYWOOD CT &amp; DEAD END;  LOWER POTTSGROVE; S...</td>\n",
       "      <td>EMS: DIZZINESS</td>\n",
       "      <td>LOWER POTTSGROVE</td>\n",
       "      <td>2015-12-10 16:56:52</td>\n",
       "      <td>2015-12-10 16:56:52-05:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                desc                    title  \\\n",
       "0  REINDEER CT & DEAD END;  NEW HANOVER; Station ...   EMS: BACK PAINS/INJURY   \n",
       "1  BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP...  EMS: DIABETIC EMERGENCY   \n",
       "2  HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...      Fire: GAS-ODOR/LEAK   \n",
       "3  AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A;...   EMS: CARDIAC EMERGENCY   \n",
       "4  CHERRYWOOD CT & DEAD END;  LOWER POTTSGROVE; S...           EMS: DIZZINESS   \n",
       "\n",
       "                 twp           timestamp           timestamp_local  \n",
       "0        NEW HANOVER 2015-12-10 17:10:52 2015-12-10 17:10:52-05:00  \n",
       "1  HATFIELD TOWNSHIP 2015-12-10 17:29:21 2015-12-10 17:29:21-05:00  \n",
       "2         NORRISTOWN 2015-12-10 14:39:21 2015-12-10 14:39:21-05:00  \n",
       "3         NORRISTOWN 2015-12-10 16:47:36 2015-12-10 16:47:36-05:00  \n",
       "4   LOWER POTTSGROVE 2015-12-10 16:56:52 2015-12-10 16:56:52-05:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data[\"timestamp_local\"] = example_data[\"timestamp\"].dt.tz_localize(\"US/Eastern\", ambiguous=True)\n",
    "example_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-eleven",
   "metadata": {},
   "source": [
    "## Which week has the most 911 calls? \n",
    "\n",
    "Next we get the week (and year) for each entry in our dataset and aggregate the data up to week and year level to identify the week with the highest volume of 911 calls.\n",
    "\n",
    "From here we will get the week commencing value for the highest volume week by getting the miniumum date within that week.\n",
    "\n",
    "In this dataset, the week with the highest volume of 911 calls was w/c 2018-02-26 with 5,075 calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excessive-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestamp_local_week  timestamp_local_year  title\n",
      "42                      9                  2018   5075\n",
      "213                    46                  2018   4116\n",
      "2                       1                  2018   3964\n",
      "114                    23                  2020   3801\n",
      "10                      3                  2016   3539\n",
      "\n",
      "\n",
      "The week with the highest volume was w/c 2018-02-26 with 5075 calls\n"
     ]
    }
   ],
   "source": [
    "example_data[\"timestamp_local_date\"] = example_data[\"timestamp_local\"].dt.date\n",
    "example_data[\"timestamp_local_week\"] = example_data[\"timestamp_local\"].dt.isocalendar().week\n",
    "example_data[\"timestamp_local_year\"] = example_data[\"timestamp_local\"].dt.year\n",
    "weekly_counts = (\n",
    "    example_data.groupby([\"timestamp_local_week\", \"timestamp_local_year\"])[\"title\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"title\", ascending=False)\n",
    ")\n",
    "\n",
    "print(weekly_counts.head())\n",
    "\n",
    "# now lets convert that week number to an actual date\n",
    "highest_week_num = weekly_counts[\"timestamp_local_week\"].iloc[0]\n",
    "highest_week_year = weekly_counts[\"timestamp_local_year\"].iloc[0]\n",
    "highest_week_volume = weekly_counts[\"title\"].iloc[0]\n",
    "week_commencing = example_data.loc[\n",
    "    (example_data[\"timestamp_local_week\"] == highest_week_num) &\n",
    "    (example_data[\"timestamp_local_year\"] == highest_week_year),\n",
    "    \"timestamp_local_date\"\n",
    "].min()\n",
    "\n",
    "print(f\"\\n\\nThe week with the highest volume was w/c {week_commencing} with {highest_week_volume} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-permit",
   "metadata": {},
   "source": [
    "## Which 30 minute window over that week had the most 911 calls?\n",
    "\n",
    "From here, we filter down our dataset to the week with the highest volume of 911 calls and bucket this subset of data into 30 minute intervals. Using these intervals we can aggregate the data up to 30 minute level and identify the window with the highest volume of 911 calls.\n",
    "\n",
    "Notice here that we use the `ceil` function to round each timestamp up to the nearest 30 minute interval and from here we subtract a `timedelta` of 30 minutes to get the start time for the interval with the highest volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaptive-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          time_interval_end  title\n",
      "0 2018-03-02 16:30:00-05:00    156\n",
      "1 2018-03-02 16:00:00-05:00    141\n",
      "2 2018-03-02 17:30:00-05:00    129\n",
      "3 2018-03-02 17:00:00-05:00    128\n",
      "4 2018-03-02 15:30:00-05:00    126\n",
      "\n",
      "\n",
      "The week with the highest volume was between 02-Mar-18 16:00 and 02-Mar-18 16:30 with 156 calls\n"
     ]
    }
   ],
   "source": [
    "# filter down to the highest week\n",
    "week_data = example_data.loc[\n",
    "    (example_data[\"timestamp_local_week\"] == highest_week_num) &\n",
    "    (example_data[\"timestamp_local_year\"] == highest_week_year)    \n",
    "].copy()\n",
    "\n",
    "# round time up to next 30 minute interval\n",
    "week_data[\"time_interval_end\"] = week_data[\"timestamp_local\"].dt.ceil(\"30min\")\n",
    "week_data_aggregated = (\n",
    "    week_data.groupby(\"time_interval_end\")[\"title\"].count().sort_values(ascending=False).reset_index()\n",
    ")\n",
    "\n",
    "print(week_data_aggregated.head())\n",
    "\n",
    "# get the time range for the highest interval format nicely for a print\n",
    "week_data_aggregated[\"time_interval_start\"] = week_data_aggregated[\"time_interval_end\"] - pd.to_timedelta(30,'m')\n",
    "highest_volume_start = week_data_aggregated[\"time_interval_start\"].dt.strftime('%d-%b-%y %H:%M').iloc[0]\n",
    "highest_volume_end = week_data_aggregated[\"time_interval_end\"].dt.strftime('%d-%b-%y %H:%M').iloc[0]\n",
    "highest_volume = week_data_aggregated[\"title\"].iloc[0]\n",
    "\n",
    "print(\n",
    "    f\"\\n\\nThe week with the highest volume was between {highest_volume_start} and {highest_volume_end}\"\\\n",
    "    f\" with {highest_volume} calls\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-cartridge",
   "metadata": {},
   "source": [
    "## What was driving the volume at the time interval with the highest volume?\n",
    "\n",
    "To figure out what was driving the volume at this time we can simply filter our dataset to that time interval and perform a `groupby` and `sum` on the title of each 911 call.\n",
    "\n",
    "Looking at the output below it looks like there was some sort of traffic accident or fire which caused a road obstruction (maybe on a busy road) which drove contacts during that 30 minute interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pressed-queensland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "Traffic: ROAD OBSTRUCTION -             46\n",
       "Traffic: VEHICLE ACCIDENT -             44\n",
       "Traffic: DISABLED VEHICLE -             17\n",
       "Fire: ELECTRICAL FIRE OUTSIDE           14\n",
       "Fire: FIRE INVESTIGATION                 6\n",
       "Fire: FIRE ALARM                         5\n",
       "EMS: VEHICLE ACCIDENT                    5\n",
       "Fire: VEHICLE ACCIDENT                   4\n",
       "Fire: BUILDING FIRE                      3\n",
       "Traffic: HAZARDOUS ROAD CONDITIONS -     2\n",
       "Name: timestamp, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busy_time = week_data.loc[week_data[\"time_interval_end\"] == highest_volume_end]\n",
    "busy_time.groupby(\"title\")[\"timestamp\"].count().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-eugene",
   "metadata": {},
   "source": [
    "## Plot all 30 minute intervals within that week \n",
    "\n",
    "Before we can plot all 30 minute intervals for the week, we must first create a dataframe containing all possible 30 minute intervals. We need to do this to account for any 30 minute intervals in our dataset where there were no 911 calls received.\n",
    "\n",
    "Once this is done, we can just left join our dataset of aggregated 30 minute data onto the dataframe of time intervals using the `time_interval_end` column before filling any missing values with 0 (this account for any missing intervals).\n",
    "\n",
    "From here we can just use pandas built in `plot` function to plot the `time_interval` column against the `title` in our aggregated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "thorough-bracket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a set of all possible timestamps\n",
    "intervals = pd.DataFrame(\n",
    "    pd.date_range(\n",
    "        start=week_data_aggregated[\"time_interval_start\"].min(),\n",
    "        end=week_data_aggregated[\"time_interval_end\"].max(),\n",
    "        freq=\"30min\"\n",
    "    ),\n",
    "    columns=[\"time_interval_end\"]\n",
    ")\n",
    "\n",
    "# join on our data based on time_interval_end (filna handles any missing time intervals)\n",
    "intervals = intervals.merge(\n",
    "    week_data_aggregated, on=[\"time_interval_end\"], how=\"left\"\n",
    ")[[\"time_interval_end\", \"title\"]].fillna(0)\n",
    "\n",
    "ax = intervals.plot.bar(x=\"time_interval_end\", y=\"title\")\n",
    "ax.set_xticklabels([])\n",
    "ax.set_ylabel(\"Volume of calls\")\n",
    "ax.get_legend().remove()\n",
    "ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-investigator",
   "metadata": {},
   "source": [
    "## Output to CSV \n",
    "\n",
    "Finally, we can create the required csv extract by first using the `tz_convert` function to convert our timestamps from US Eastern to UTC before using the `apply` method to convert these UTC timestamps to ISO format.\n",
    "\n",
    "With the time conversion and formatting complete, we can simply use pandas `to_csv` method to save our selected columns to csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competent-astronomy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_iso</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>twp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-10T22:10:52+00:00</td>\n",
       "      <td>EMS: BACK PAINS/INJURY</td>\n",
       "      <td>REINDEER CT &amp; DEAD END;  NEW HANOVER; Station ...</td>\n",
       "      <td>NEW HANOVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-10T22:29:21+00:00</td>\n",
       "      <td>EMS: DIABETIC EMERGENCY</td>\n",
       "      <td>BRIAR PATH &amp; WHITEMARSH LN;  HATFIELD TOWNSHIP...</td>\n",
       "      <td>HATFIELD TOWNSHIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-10T19:39:21+00:00</td>\n",
       "      <td>Fire: GAS-ODOR/LEAK</td>\n",
       "      <td>HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-10T21:47:36+00:00</td>\n",
       "      <td>EMS: CARDIAC EMERGENCY</td>\n",
       "      <td>AIRY ST &amp; SWEDE ST;  NORRISTOWN; Station 308A;...</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-10T21:56:52+00:00</td>\n",
       "      <td>EMS: DIZZINESS</td>\n",
       "      <td>CHERRYWOOD CT &amp; DEAD END;  LOWER POTTSGROVE; S...</td>\n",
       "      <td>LOWER POTTSGROVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp_iso                    title  \\\n",
       "0  2015-12-10T22:10:52+00:00   EMS: BACK PAINS/INJURY   \n",
       "1  2015-12-10T22:29:21+00:00  EMS: DIABETIC EMERGENCY   \n",
       "2  2015-12-10T19:39:21+00:00      Fire: GAS-ODOR/LEAK   \n",
       "3  2015-12-10T21:47:36+00:00   EMS: CARDIAC EMERGENCY   \n",
       "4  2015-12-10T21:56:52+00:00           EMS: DIZZINESS   \n",
       "\n",
       "                                                desc                twp  \n",
       "0  REINDEER CT & DEAD END;  NEW HANOVER; Station ...        NEW HANOVER  \n",
       "1  BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP...  HATFIELD TOWNSHIP  \n",
       "2  HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...         NORRISTOWN  \n",
       "3  AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A;...         NORRISTOWN  \n",
       "4  CHERRYWOOD CT & DEAD END;  LOWER POTTSGROVE; S...   LOWER POTTSGROVE  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data[\"timestamp_utc\"] = example_data[\"timestamp_local\"].dt.tz_convert(pytz.utc)\n",
    "example_data[\"timestamp_iso\"] = example_data[\"timestamp_utc\"].apply(lambda x: x.isoformat())\n",
    "\n",
    "output = example_data[[\"timestamp_iso\", \"title\", \"desc\", \"twp\"]]\n",
    "output.to_csv(\"sample_output.csv\", index=False)\n",
    "output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
